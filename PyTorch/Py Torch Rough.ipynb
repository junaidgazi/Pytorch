{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71c45eb-c5b2-42ff-9eb9-6c2156e6ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b081386c-7005-457d-8cea-8c4f982ce1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5565, 0.1442],\n",
      "        [0.3275, 0.4961]])\n",
      "tensor([[0.7192, 0.0033],\n",
      "        [0.3463, 0.6491]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(2,2)\n",
    "y=torch.rand(2,2)\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8253984-a4f7-458b-a522-5c85e45cf561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7725, 0.6757, 0.9823],\n",
      "        [0.3921, 0.9730, 0.0306],\n",
      "        [0.7616, 0.1651, 0.0770],\n",
      "        [0.4046, 0.6002, 0.4845],\n",
      "        [0.5866, 0.6888, 0.0480]])\n",
      "0.9730198979377747\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[1,1].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a135bda-8c58-4d33-843c-6fcbcdbcefd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9731, 0.1445, 0.9626, 0.1830],\n",
      "        [0.0038, 0.4315, 0.0374, 0.4325],\n",
      "        [0.0536, 0.8830, 0.5563, 0.8110],\n",
      "        [0.3809, 0.4717, 0.8330, 0.0883]])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(4,4)\n",
    "print(x)\n",
    "y=x.view(-1,16)\n",
    "print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a1be58-b21e-446e-9421-5cfbf177ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abf37cdd-5b42-4151-808e-e32de37641c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(5)\n",
    "print(a)\n",
    "b=a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db9c43b1-7612-4381-9063-69627692f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe7eddfb-fe4a-41d4-a092-803b52a3dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a=np.ones(5)\n",
    "print(a)\n",
    "b=torch.from_numpy(a)\n",
    "print(b)\n",
    "\n",
    "\n",
    "a+=1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cea72527-1d36-4141-b4d5-c8746b5400ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    devivce=torch.device(\"cuda\")\n",
    "    x=torch.ones(5, device=device)\n",
    "    y=torch.ones(5)\n",
    "    y=y.to(device)\n",
    "    z=x+y\n",
    "    z=z.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e4852d4-a489-4d4d-aff5-7fe3bdccb1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x=torch.ones(5,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82e885c0-7540-4d60-8f2e-3a2a98bd277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1e3f6f8-8e95-4cd9-a49a-174f0ab3116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2673, 0.8358, 0.8933], requires_grad=True)\n",
      "tensor([2.2673, 2.8358, 2.8933], grad_fn=<AddBackward0>)\n",
      "tensor([11.6549, 22.8042, 24.2207], grad_fn=<MulBackward0>)\n",
      "tensor(19.5599, grad_fn=<MeanBackward0>)\n",
      "tensor([5.1405, 8.0416, 8.3713])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(3, requires_grad=True)\n",
    "print(x)\n",
    "y=x+2\n",
    "print(y)\n",
    "\n",
    "z=y*y**2\n",
    "print(z)\n",
    "\n",
    "z=z.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward() #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9c500dd-1183-4900-82e8-9dc6ef3be936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1282, 0.8943, 0.0735], requires_grad=True)\n",
      "tensor([2.1282, 2.8943, 2.0735], grad_fn=<AddBackward0>)\n",
      "tensor([ 9.6392, 24.2450,  8.9154], grad_fn=<MulBackward0>)\n",
      "tensor(14.2665, grad_fn=<MeanBackward0>)\n",
      "tensor([4.5293, 8.3769, 4.2996])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(3, requires_grad=True)\n",
    "print(x)\n",
    "y=x+2\n",
    "print(y)\n",
    "\n",
    "z=y*y**2\n",
    "print(z)\n",
    "\n",
    "z=z.mean()\n",
    "print(z)\n",
    "\n",
    "#z.backward() #dz/dx\n",
    "#print(x.grad)\n",
    "\n",
    "v= torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68ce08a6-41f4-41e7-be53-262963498530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6092, 0.2558, 0.6440], requires_grad=True)\n",
      "tensor([0.6092, 0.2558, 0.6440])\n",
      "tensor([0.6092, 0.2558, 0.6440])\n",
      "tensor([2.6092, 2.2558, 2.6440])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#x.require_grad_(False)\n",
    "#x.detach()\n",
    "#with torch.no_grad():\n",
    "\n",
    "x.requires_grad_(False)\n",
    "print(x)\n",
    "\n",
    "x.detach()\n",
    "print(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y=x+2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcff78d3-3719-4aaa-8ec5-632ceb282866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights=torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    model_output=(weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982bdef-d926-46fe-ba9e-3ef60bfa403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=torch.ones(4, requires_grad=True)\n",
    "\n",
    "optimizer=torch.optim.SGD(weights, lr=0.01)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f355a-a59d-4e35-b376-c12c62bdb081",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=torch.ones(4, requires_grad=True)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5fbd5ba-317a-4226-987e-0221f7d80b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.0)\n",
    "y=torch.tensor(2.0)\n",
    "\n",
    "w=torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#forword pass and compute the loss\n",
    "y_hat=w*x\n",
    "loss=(y_hat-y)**2\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "#update  weights\n",
    "# next  weckwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6411adfb-4fe4-477f-95c4-9d36535b6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a8b6c01f-9bc0-4014-9ca5-4271fc1bbcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before trainning:f(5)=0.000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m n_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iters):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m#prediction= forward pass\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     y_pred\u001b[38;5;241m=\u001b[39mforward(x)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m#loss\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     l\u001b[38;5;241m=\u001b[39mloss(Y, y_pred)\n",
      "Cell \u001b[1;32mIn[77], line 8\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(x):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w\u001b[38;5;241m*\u001b[39mx\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:1062\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "X=torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y=torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#model predictions\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss\n",
    "def loss(y, y_predicted):\n",
    "    return((y_predicted-y)**2).mean()\n",
    "\n",
    "#gradient\n",
    "#MSE = 1/N * (w*x - y)**2\n",
    "#dJ/dw= 1/N 2x (w*x -y)\n",
    "\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return torch.dot(2*x, y_predicted-y).mean()\n",
    "print(f'prediction before trainning:f(5)={forward(5):.3f}')\n",
    "\n",
    "#trainning\n",
    "learning_rate=0.01\n",
    "n_iters=10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction= forward pass\n",
    "    y_pred=forward(x)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y, y_pred)\n",
    "\n",
    "    #gradients=backward pass\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w-=learning_rate*w.grad\n",
    "\n",
    "    #zero gradients\n",
    "    w.grad.zero_()\n",
    "    if epoch % 1==0:\n",
    "        print(f'epoch {epoch+1}: w={w:.3f}, loss={l:.8f}')\n",
    "\n",
    "print(f'prediction after trainning:f(5)={forward(5):.3f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "66213157-4e3e-4c94-b823-a6f9beb232e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before trainning:f(5)=0.000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m n_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iters):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m#prediction= forward pass\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     y_pred\u001b[38;5;241m=\u001b[39mforward(x)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m#loss\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     l\u001b[38;5;241m=\u001b[39mloss(Y, y_pred)\n",
      "Cell \u001b[1;32mIn[82], line 8\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(x):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w \u001b[38;5;241m*\u001b[39m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:1062\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "X=torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y=torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "#model predictions\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "#loss\n",
    "def loss(y, y_predicted):\n",
    "    return((y_predicted-y)**2).mean()\n",
    "\n",
    "#gradient\n",
    "#MSE = 1/N * (w*x - y)**2\n",
    "#dJ/dw= 1/N 2x (w*x -y)\n",
    "\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return torch.dot(2*x, y_predicted-y).mean()\n",
    "print(f'prediction before trainning:f(5)={forward(5):.3f}')\n",
    "\n",
    "#trainning\n",
    "learning_rate=0.01\n",
    "n_iters=10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction= forward pass\n",
    "    y_pred=forward(x)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y, y_pred)\n",
    "\n",
    "    #gradients=backward pass\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w-=learning_rate*w.detach().numpy()\n",
    "\n",
    "    #zero gradients\n",
    "    w.grad.zero_()\n",
    "    if epoch % 1==0:\n",
    "        print(f'epoch {epoch+1}: w={w:.3f}, loss={l:.8f}')\n",
    "\n",
    "print(f'prediction after trainning:f(5)={forward(5):.3f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59c1da23-7c2d-4063-9fa0-2a383669058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "prediction before trainning:f(5)=3.583\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 44\u001b[0m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iters):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m#prediction= forward pass\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     y_pred\u001b[38;5;241m=\u001b[39mmodel(x)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#loss\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     l\u001b[38;5;241m=\u001b[39mloss(Y, y_pred)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "#1) Design model (input, output size, forward pass)\n",
    "#2) construct loss and optimizer\n",
    "#3) Trainning loop\n",
    "#- forward pass: compute prediction\n",
    "#- backward pass: gradients\n",
    "# - update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X=torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y=torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test=torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features=X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size=n_features\n",
    "output_size=n_features\n",
    "model=nn.Linear(input_size,output_size)\n",
    "    \n",
    "print(f'prediction before trainning:f(5)={model(X_test).item():.3f}')\n",
    "\n",
    "#gradient\n",
    "#MSE = 1/N * (w*x - y)**2\n",
    "#dJ/dw= 1/N 2x (w*x -y)\n",
    "\n",
    "\n",
    "#def gradient(x, y, y_predicted):\n",
    " #   return torch.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "\n",
    "#trainning\n",
    "learning_rate=0.01\n",
    "n_iters=100\n",
    "\n",
    "#loss\n",
    "loss=nn.MSELoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction= forward pass\n",
    "    y_pred=model(x)\n",
    "\n",
    "    #loss\n",
    "    l=loss(Y, y_pred)\n",
    "\n",
    "    #gradients=backward pass\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    if epoch % 1==0:\n",
    "        [w,b]= model.parameters()\n",
    "        print(f'epoch {epoch+1}: w={w[0][0].item():.3f}, loss={l:.8f}')\n",
    "\n",
    "print(f'prediction after trainning:f(5)={model(X_test).item():.3f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5c0acbcc-f4f5-4abe-a3f9-ac3223453446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n",
      "Prediction before training: f(5) = 0.790\n",
      "epoch  1 : w =  0.561310350894928  loss =  tensor(27.6783, grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  1.7662389278411865  loss =  tensor(0.7161, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  1.9601197242736816  loss =  tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  1.9913727045059204  loss =  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  1.9964649677276611  loss =  tensor(2.2024e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  1.9973474740982056  loss =  tensor(9.3828e-06, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  1.9975509643554688  loss =  tensor(8.5433e-06, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  1.9976434707641602  loss =  tensor(8.0382e-06, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  1.9977163076400757  loss =  tensor(7.5701e-06, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  1.9977842569351196  loss =  tensor(7.1301e-06, grad_fn=<MseLossBackward0>)\n",
      "Prediction after training: f(5) = 9.996\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input, output, forward pass with different layers)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#       - Forward = compute prediction and loss\n",
    "#       - Backward = compute gradients\n",
    "#       - Update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples, watch the shape!\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "'''\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "'''\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88618a62-36f5-451e-a70f-708b3f17dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "  import torch\n",
    "  import torch.nn as nn\n",
    "  import numpy as np\n",
    "  import sklearn import datasets\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  x_numpy,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
